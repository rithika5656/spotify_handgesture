# ğŸ—ï¸ ARCHITECTURE DOCUMENTATION

## Project Overview Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 GESTURE-CONTROLLED MEDIA PLAYER                 â”‚
â”‚                      (AI + Computer Vision)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                           SYSTEM PIPELINE
                           
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                   â”‚
â”‚  ğŸ“¸ INPUT: Webcam Feed (30-60 FPS)                              â”‚
â”‚                    â†“                                             â”‚
â”‚  MODULE 1: Hand Detection (MediaPipe Hands)                     â”‚
â”‚  â”œâ”€ Detects hand presence in frame                             â”‚
â”‚  â”œâ”€ Extracts 21 3D landmark points                             â”‚
â”‚  â”œâ”€ Returns: landmarks_list (n_hands, 21, 3)                  â”‚
â”‚                    â†“                                             â”‚
â”‚  MODULE 2: Feature Extraction                                   â”‚
â”‚  â”œâ”€ Calculates distances between landmarks                     â”‚
â”‚  â”œâ”€ Computes angles between joints                             â”‚
â”‚  â”œâ”€ Determines finger states                                   â”‚
â”‚  â”œâ”€ Returns: feature_vector (1, 8-15)                         â”‚
â”‚                    â†“                                             â”‚
â”‚  MODULE 3: Gesture Classification (ML Model)                    â”‚
â”‚  â”œâ”€ Uses trained Random Forest/SVM/NN                          â”‚
â”‚  â”œâ”€ Predicts gesture class (0-4)                               â”‚
â”‚  â”œâ”€ Returns: gesture_class, confidence                         â”‚
â”‚                    â†“                                             â”‚
â”‚  Confidence Filtering (threshold = 0.6)                         â”‚
â”‚  â”œâ”€ If confidence < threshold: SKIP action                     â”‚
â”‚  â”œâ”€ If confidence â‰¥ threshold: PROCEED                         â”‚
â”‚                    â†“                                             â”‚
â”‚  Gesture Smoothing (history buffer)                             â”‚
â”‚  â”œâ”€ Keep last 5 predictions                                    â”‚
â”‚  â”œâ”€ Use majority vote for stability                            â”‚
â”‚                    â†“                                             â”‚
â”‚  MODULE 4: Action Mapping                                       â”‚
â”‚  â”œâ”€ Map gesture â†’ media action                                 â”‚
â”‚  â”œâ”€ Returns: action (volume_up, play_pause, etc.)             â”‚
â”‚                    â†“                                             â”‚
â”‚  Action Cooldown (0.3 seconds)                                  â”‚
â”‚  â”œâ”€ Prevent duplicate action triggering                        â”‚
â”‚  â”œâ”€ Allow only one action per 0.3 seconds                      â”‚
â”‚                    â†“                                             â”‚
â”‚  MODULE 5: Media Controller                                     â”‚
â”‚  â”œâ”€ Execute volume control (pycaw)                             â”‚
â”‚  â”œâ”€ Send media key presses (pyautogui)                         â”‚
â”‚  â”œâ”€ Update system state                                        â”‚
â”‚                    â†“                                             â”‚
â”‚  MODULE 6: UI Renderer                                          â”‚
â”‚  â”œâ”€ Display gesture name                                       â”‚
â”‚  â”œâ”€ Show confidence score                                      â”‚
â”‚  â”œâ”€ Draw volume indicator                                      â”‚
â”‚  â”œâ”€ Show FPS counter                                           â”‚
â”‚                    â†“                                             â”‚
â”‚  ğŸ”Š OUTPUT: Volume & Media Control                              â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Module Dependency Graph

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Webcam     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ hand_detection  â”‚â—„â”€â”€â”€ MediaPipe
                    â”‚   MODULE 1      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ landmarks[0] = 21 points
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚feature_extraction   â”‚
                    â”‚   MODULE 2          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ features[8]
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚gesture_classifier   â”‚â—„â”€â”€â”€ scikit-learn
                    â”‚   MODULE 3          â”‚     (trained model)
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ gesture_class, confidence
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  action_mapper      â”‚â—„â”€â”€â”€ config.py
                    â”‚   MODULE 4          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ action (string)
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ media_controller    â”‚â—„â”€â”€â”€ pycaw, pyautogui
                    â”‚   MODULE 5          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Volume Control â”‚   â”‚ Media Control  â”‚
        â”‚   (pycaw)      â”‚   â”‚   (pyautogui)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ System Audio &  â”‚
                    â”‚    Media Apps   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow Diagram

```
FRAME PROCESSING PIPELINE (per frame):

Input Frame (BGR, 640x480)
    â”‚
    â”œâ”€â–º MediaPipe Hand Detection
    â”‚   â”œâ”€ Hand Present? NO â”€â”€â–º Skip to next frame
    â”‚   â””â”€ Hand Present? YES â”€â”
    â”‚                         â”‚
    â”‚     Extract 21 Landmarks (x, y, z) per hand
    â”‚     â””â”€ landmarks: array(21, 3)
    â”‚
    â”œâ”€â–º Feature Extraction
    â”‚   â”œâ”€ Thumb-Index distance
    â”‚   â”œâ”€ All finger distances
    â”‚   â”œâ”€ Number of open fingers
    â”‚   â”œâ”€ Wrist position
    â”‚   â”œâ”€ Joint angles
    â”‚   â””â”€ features: array(8,)
    â”‚
    â”œâ”€â–º Gesture Classification
    â”‚   â”œâ”€ Scale features (normalize)
    â”‚   â”œâ”€ Feed to ML model
    â”‚   â”œâ”€ Get predictions for all classes
    â”‚   â”œâ”€ Select highest probability
    â”‚   â””â”€ gesture_class: 0-4, confidence: 0.0-1.0
    â”‚
    â”œâ”€ Confidence > threshold? NO â”€â”€â–º Skip action
    â”‚  \ YES
    â”‚
    â”œâ”€â–º Gesture Smoothing
    â”‚   â”œâ”€ Add to history buffer (size=5)
    â”‚   â”œâ”€ Majority vote
    â”‚   â””â”€ stable_gesture: 0-4
    â”‚
    â”œâ”€â–º Action Mapping
    â”‚   â”œâ”€ Look up: gesture â†’ action
    â”‚   â””â”€ action: 'volume_up' | 'play_pause' | etc.
    â”‚
    â”œâ”€ Cooldown elapsed? NO â”€â”€â–º Skip execution
    â”‚  \ YES
    â”‚
    â”œâ”€â–º Media Control Execution
    â”‚   â””â”€ Execute system action
    â”‚
    â””â”€â–º UI Rendering
        â”œâ”€ Draw gesture name
        â”œâ”€ Show confidence %
        â”œâ”€ Display volume bar
        â”œâ”€ Show FPS counter
        â””â”€ Display frame


TIMING ANALYSIS (per frame @ 30 FPS = 33.3ms per frame):

                                    Time (ms)    Budget
    â”Œâ”€ Hand Detection           2-5 ms       â—„â”€ ~17%
    â”œâ”€ Feature Extraction       1-2 ms       â—„â”€ ~5%
    â”œâ”€ Model Prediction         1-3 ms       â—„â”€ ~9%
    â”œâ”€ Gesture Smoothing        <1 ms        â—„â”€ ~3%
    â”œâ”€ Action Mapping           <1 ms        â—„â”€ ~3%
    â”œâ”€ Media Control            <1 ms        â—„â”€ ~3%
    â”œâ”€ UI Rendering             3-5 ms       â—„â”€ ~15%
    â””â”€ Buffer/Display          5-10 ms       â—„â”€ ~30%
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TOTAL:                 15-30 ms      (30-60 FPS achieved)
```

---

## Class Hierarchy

```
HandDetector
â”œâ”€ Attributes:
â”‚  â”œâ”€ hands (MediaPipe Hands object)
â”‚  â”œâ”€ mp_hands (MediaPipe solutions)
â”‚  â””â”€ max_hands, detection_con, tracking_con
â”‚
â””â”€ Methods:
   â”œâ”€ detect_hands(frame) â†’ (frame, landmarks_list)
   â””â”€ get_hand_position(landmarks) â†’ (x_min, y_min, x_max, y_max, w, h)


FeatureExtractor
â”œâ”€ Attributes:
â”‚  â””â”€ landmarks (current hand landmarks)
â”‚
â””â”€ Methods:
   â”œâ”€ distance(p1, p2) â†’ float
   â”œâ”€ angle_between_points(p1, p2, p3) â†’ float (degrees)
   â”œâ”€ is_finger_open(tip, pip) â†’ bool
   â”œâ”€ extract_features(landmarks) â†’ array(8,)
   â””â”€ extract_all_distances(landmarks) â†’ array(n,)


GestureClassifier
â”œâ”€ Attributes:
â”‚  â”œâ”€ model (SVM/RF/MLP)
â”‚  â”œâ”€ scaler (StandardScaler)
â”‚  â”œâ”€ model_type (string)
â”‚  â””â”€ is_trained (bool)
â”‚
â””â”€ Methods:
   â”œâ”€ train(X_train, y_train)
   â”œâ”€ predict(features) â†’ (gesture_class, confidence)
   â”œâ”€ predict_batch(X) â†’ (predictions, confidences)
   â”œâ”€ save_model(path)
   â”œâ”€ load_model(path)
   â””â”€ get_gesture_name(class) â†’ string


ActionMapper
â”œâ”€ Attributes:
â”‚  â”œâ”€ mapping (dict: gesture â†’ action)
â”‚  â”œâ”€ last_action
â”‚  â””â”€ action_counter
â”‚
â””â”€ Methods:
   â”œâ”€ get_action(gesture) â†’ string
   â”œâ”€ set_custom_mapping(gesture, action)
   â”œâ”€ reset_mapping()
   â”œâ”€ get_mapping() â†’ dict
   â””â”€ get_action_description(action) â†’ string


MediaController
â”œâ”€ Attributes:
â”‚  â”œâ”€ volume_control (Windows audio API)
â”‚  â”œâ”€ current_volume
â”‚  â”œâ”€ last_action_time
â”‚  â””â”€ action_cooldown
â”‚
â””â”€ Methods:
   â”œâ”€ get_volume() â†’ float (0.0-1.0)
   â”œâ”€ set_volume(volume)
   â”œâ”€ increase_volume(step)
   â”œâ”€ decrease_volume(step)
   â”œâ”€ play_pause()
   â”œâ”€ next_track()
   â”œâ”€ previous_track()
   â”œâ”€ execute_action(action)
   â””â”€ _check_cooldown() â†’ bool


GestureControlPipeline
â”œâ”€ Attributes:
â”‚  â”œâ”€ hand_detector
â”‚  â”œâ”€ feature_extractor
â”‚  â”œâ”€ gesture_classifier
â”‚  â”œâ”€ action_mapper
â”‚  â”œâ”€ media_controller
â”‚  â”œâ”€ gesture_history
â”‚  â””â”€ confidence_threshold
â”‚
â””â”€ Methods:
   â”œâ”€ process_frame(frame) â†’ (frame, gesture, confidence)
   â”œâ”€ run(camera_id)
   â”œâ”€ _draw_ui(frame, gesture, confidence, action)
   â””â”€ _draw_settings(frame)
```

---

## File Organization

```
HAND GESTURE/
â”‚
â”œâ”€â”€ ğŸ“ modules/                          # Core modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hand_detection.py               (Module 1)
â”‚   â”œâ”€â”€ feature_extraction.py           (Module 2)
â”‚   â”œâ”€â”€ gesture_classifier.py           (Module 3)
â”‚   â”œâ”€â”€ action_mapper.py                (Module 4)
â”‚   â””â”€â”€ media_controller.py             (Module 5)
â”‚
â”œâ”€â”€ ğŸ“ data/                            # Training data and models
â”‚   â”œâ”€â”€ 0_PALM/
â”‚   â”‚   â”œâ”€â”€ features_0.npy
â”‚   â”‚   â”œâ”€â”€ landmarks_0.npy
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ 1_FIST/
â”‚   â”œâ”€â”€ 2_PINCH/
â”‚   â”œâ”€â”€ 3_POINT/
â”‚   â”œâ”€â”€ 4_V_SIGN/
â”‚   â”œâ”€â”€ gesture_model_random_forest.pkl
â”‚   â””â”€â”€ gesture_model_random_forest_scaler.pkl
â”‚
â”œâ”€â”€ ğŸ collect_data.py                 (Data collection)
â”œâ”€â”€ ğŸ train_model.py                  (Training script)
â”œâ”€â”€ ğŸ main_pipeline.py                (Module 6 - Real-time control)
â”œâ”€â”€ ğŸ test_modules.py                 (Testing utility)
â”œâ”€â”€ ğŸ quick_start.py                  (Setup wizard)
â”œâ”€â”€ ğŸ config.py                       (Configuration)
â”œâ”€â”€ ğŸ setup.py                        (Installation)
â”‚
â”œâ”€â”€ ğŸ“„ README.md                       (Full documentation)
â”œâ”€â”€ ğŸ“„ PROJECT_SUMMARY.md              (Project overview)
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md                 (This file)
â””â”€â”€ ğŸ“„ requirements.txt                (Dependencies)
```

---

## Integration Points

### 1. Camera Input
- OpenCV (`cv2.VideoCapture`)
- Feeds raw frames to HandDetector

### 2. Hand Detection
- MediaPipe Hands API
- Outputs landmark coordinates

### 3. ML Inference
- Scikit-learn models (Random Forest/SVM/MLP)
- Takes extracted features, returns prediction

### 4. Media Control
- Windows API (pycaw for volume)
- System keyboard (pyautogui for media keys)
- Sends commands to OS

### 5. UI Display
- OpenCV drawing functions
- Displays live feedback

---

## Performance Characteristics

| Component | Latency | Throughput |
|-----------|---------|-----------|
| Hand Detection | 3-5ms | 60 FPS |
| Feature Extraction | 1-2ms | 500+ FPS |
| ML Prediction | 1-3ms | 300+ FPS |
| Media Control | <1ms | 1000+ ops/sec |
| **Total (per frame)** | **15-30ms** | **30-60 FPS** |

---

## Error Handling

```
Main Pipeline Error Flow:

â”Œâ”€ Camera Error
â”œâ”€â†’ Caught: Display error message
â”œâ”€â†’ Recovery: Retry camera initialization

â”Œâ”€ Model Not Found
â”œâ”€â†’ Caught: FileNotFoundError
â”œâ”€â†’ Recovery: Prompt to train model first

â”Œâ”€ Low Confidence Prediction
â”œâ”€â†’ Caught: Check threshold
â”œâ”€â†’ Recovery: Skip action, continue

â”Œâ”€ Audio Control Error
â”œâ”€â†’ Caught: pycaw exception
â”œâ”€â†’ Recovery: Log error, continue without volume control

â”Œâ”€ Feature Extraction Error
â”œâ”€â†’ Caught: Invalid landmarks
â”œâ”€â†’ Recovery: Skip frame, get next hand
```

---

## System Requirements

- **OS:** Windows 10/11, Linux, macOS
- **Python:** 3.8+
- **RAM:** 4GB minimum (8GB recommended)
- **GPU:** Optional (CPU sufficient for 30 FPS)
- **Camera:** USB webcam, 30+ FPS capable
- **Internet:** Minimal (only for pip install)

---

This architecture is designed for:
âœ… **Modularity** - Easy to modify components  
âœ… **Extensibility** - Easy to add features  
âœ… **Performance** - Real-time processing  
âœ… **Reliability** - Error handling throughout  
âœ… **Maintainability** - Clean, documented code  
